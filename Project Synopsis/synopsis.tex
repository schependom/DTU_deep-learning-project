\documentclass[dtu]{dtuarticle}
\usepackage{parskip} % use enters instead of indents

\newcommand{\todo}[1]{\color{red}[TODO: #1]\color{black}}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}

\usepackage{url}
\usepackage{hyperref}

\usepackage{subcaption}

\title{Recursive Reasoning with Tiny Networks}
\subtitle{Group Project Deep Learning}
\author{Group 7}
\course{02456 Deep Learning}
\address{
	DTU Compute \\
	Fall 2025
}
\date{\today}



\begin{document}

	\maketitle

	\subsection*{Background and motivation}

	In Natural Language Processing, the dominant paradigm is the transformer. In recent years the evolution of this paradigm has involved larger and larger models, with exponential growth in parameters and required computation. Even then, these Large Language Models (LLMs) struggle with complicated tasks requiring thorough reasoning. As reasoning is handled in test-time computing, their auto-regressive token-based nature as well as context length- and self attention limitations makes reasoning fall apart as complexity is introduced. Errors compound and eventually turn into nonsense or false answers, further degrading performance. In other words, the jack-of-all-trades nature of LLMs severely hinders its potential for growth.

	\subsubsection*{Tiny Recursive Models (TRMs)}

	Joliceur-Martineau \cite{paper} defines the Tiny Recursive Model (TRM). Based on a similar model (HRM) inspired by the hierarchy of the brain (Wang et al. \cite{wang2025hierarchicalreasoningmodel}), the TRM attempts to solve the above mentioned issues by handling reasoning latently, rather than in lossy token representation. The TRM handles reasoning entirely in its latent space, by recursively updating its latent variable through multiple supervision steps. This allows complex representation of the problem and reasoning state. Additionally, the paper shows that all of this can be handled with tiny models - at least relative to the ordinary size of an LLM. This represents a potential new paradigm for reasoning in language modeling.

	\subsection*{Milestones}

	We aim to accomplish the following:

	\begin{description}
		% 	\item[Verifying the replacement of attention by MLP's]
		% 	Self-attention is good when $L \gg D$, i.e. the input length is way bigger than the embedding dimension. This is because we only need a $D \times 3D$ matrix for the keys, queries and values. But for $L \leq D$, we can use a linear map from $\mathbb{R}^{L}$ to $\mathbb{R}^{L}$ instead, which only requires an $L \times L$ matrix. This means that for small models, according to the authors, we can replace attention with MLP's.
		% 	\item[Verifying that adding more layers leads to overfitting and 2 layers is optimal]
		% 	Decreasing the number of layers, while scaling the number of recursions $n$ proportionally, the authors found that using 2 layers (instead of 4 layers) maximized generalization.
		% 	\item[Verifying the removal of the continue loss] By removing the continue loss with Adaptive computational time (ACT), there's supposedly no need for the expensive second forward pass, while still being able to determine when to
		% 	halt with relatively good accuracy.
		\item[Reproducing the main results of the original paper]
		\item[Testing the TRM on different puzzles, comparing it to other models]
		We would like to try out the Towers of Hanoi, as well as a board game, like chess. For the latter, we could evaluate moves made by the TRM using a chess engine.
		\item[Experimenting with language generation]
		As mentioned in the paper, it would be interesting to see how the TRM performs at producing natural language output.
	\end{description}

	% The authors found that $T=3$ (latent) recursion processes, each containing $n=6$ evaluations of $f$ was optimal for Sudoku-Extreme, but mentioned that they did not test more than $T \cdot n = 42$ total recursions due to limited resources. However, the authors did \textit{not} mention the specific hardware they used. We could try to increase the number of recursions, assuming DTU HPC is more powerful than the authors' resources.


	\bibliographystyle{unsrt}
	\bibliography{citations}

	\vspace{0.7cm}

	\begin{table}[h!]
		\centering
		\begin{tabular}{l | l}
			\textbf{Name} & \textbf{Study number} \\
			\hline
			Vincent Van Schependom & s251739 \\
			Malthe Bresler & s214631 \\
			Sara Maria Bj√∏rn Andersen & s202186\\
			Jacob Corkill Nielsen & s204093
		\end{tabular}
	\end{table}

\end{document}